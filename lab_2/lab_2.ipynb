{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v6vHsWgHycOc",
        "outputId": "b4d01558-0e46-4d81-8504-90ee6c316c24"
      },
      "outputs": [],
      "source": [
        "!yes | oidv6 downloader --dataset=OIDv6 --classes \"Horse\" \"Fish\" \"Monkey\" --type_data \"train\" --no_labels --limit 1000 --dataset \"data/\"\n",
        "!yes | oidv6 downloader --dataset=OIDv6 --classes \"Horse\" \"Fish\" \"Monkey\" --type_data \"validation\" --no_labels --limit 125  --dataset \"data/\"\n",
        "!yes | oidv6 downloader --dataset=OIDv6 --classes \"Horse\" \"Fish\" \"Monkey\" --type_data \"test\" --no_labels --limit 125  --dataset \"data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model properties\n",
        "NUMBER_OF_CLASSES = 3\n",
        "CLASSES = [\"Fish\", \"Horse\", \"Monkey\"]\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "NUMBER_OF_WORKERS = 4\n",
        "\n",
        "IMAGE_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEJkkrcSUWE6"
      },
      "outputs": [],
      "source": [
        "# train imports\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9SQW9HthLfs"
      },
      "outputs": [],
      "source": [
        "# train properties\n",
        "NUMBER_OF_TRAIN_IMAGES_PER_CLASS = 1000\n",
        "NUMBER_OF_VALIDATION_IMAGES_PER_CLASS = 125\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "NUMBER_OF_EPOCHS = 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XatQmH00fW0x"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalNetwork(torch.nn.Module):\n",
        "  def __init__(self, number_of_classes: int) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    # block 1\n",
        "    self.conv_1_1 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.batch_norm_1_1 = torch.nn.BatchNorm2d(num_features=32)\n",
        "    self.conv_1_2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.batch_norm_1_2 = torch.nn.BatchNorm2d(64)\n",
        "    self.pool_1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.dropout_1 = torch.nn.Dropout(p=0.25)\n",
        "\n",
        "    # block 2\n",
        "    self.conv_2_1 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.batch_norm_2_1 = torch.nn.BatchNorm2d(64)\n",
        "    self.conv_2_2 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "    self.batch_norm_2_2 = torch.nn.BatchNorm2d(128)\n",
        "    self.pool_2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.dropout_2 = torch.nn.Dropout(p=0.25)\n",
        "\n",
        "    # block 3\n",
        "    self.conv_3_1 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "    self.batch_norm_3_1 = torch.nn.BatchNorm2d(256)\n",
        "    self.conv_3_2 = torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "    self.batch_norm_3_2 = torch.nn.BatchNorm2d(256)\n",
        "    self.pool_3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.dropout_3 = torch.nn.Dropout(p=0.25)\n",
        "\n",
        "    self.adaptive_pool = torch.nn.AdaptiveAvgPool2d((8, 8))\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc1 = torch.nn.Linear(256 * 8 * 8, 1024)\n",
        "    self.dropout_fc = torch.nn.Dropout(p=0.5)\n",
        "    self.fc2 = torch.nn.Linear(1024, number_of_classes)\n",
        "\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "\n",
        "  def forward(self, x) -> None:\n",
        "    return torch.nn.Sequential(\n",
        "        self.conv_1_1,\n",
        "        self.relu,\n",
        "        self.batch_norm_1_1,\n",
        "        self.conv_1_2,\n",
        "        self.relu,\n",
        "        self.batch_norm_1_2,\n",
        "        self.pool_1,\n",
        "        self.dropout_1,\n",
        "        self.conv_2_1,\n",
        "        self.relu,\n",
        "        self.batch_norm_2_1,\n",
        "        self.conv_2_2,\n",
        "        self.relu,\n",
        "        self.batch_norm_2_2,\n",
        "        self.pool_2,\n",
        "        self.dropout_2,\n",
        "        self.conv_3_1,\n",
        "        self.relu,\n",
        "        self.batch_norm_3_1,\n",
        "        self.conv_3_2,\n",
        "        self.relu,\n",
        "        self.batch_norm_3_2,\n",
        "        self.pool_3,\n",
        "        self.dropout_3,\n",
        "        self.adaptive_pool,\n",
        "        self.flatten,\n",
        "        self.fc1,\n",
        "        self.relu,\n",
        "        self.dropout_fc,\n",
        "        self.fc2,\n",
        "        )(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "djldAzIjt-tP",
        "outputId": "ff0c858e-86db-4ab5-93bc-1f3696865504"
      },
      "outputs": [],
      "source": [
        "# calculate normalization for train dataset\n",
        "normalization_transformations = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "normalization_dataset = torchvision.datasets.ImageFolder(\n",
        "    root=\"data/train/\",\n",
        "    transform=normalization_transformations\n",
        ")\n",
        "\n",
        "normalization_dataloader = torch.utils.data.DataLoader(dataset=normalization_dataset, batch_size=BATCH_SIZE, num_workers=NUMBER_OF_WORKERS)\n",
        "channels_sum = torch.zeros(3)\n",
        "channels_sq_sum = torch.zeros(3)\n",
        "\n",
        "for images, labels in normalization_dataloader:\n",
        "    channels_sum += images.sum(dim=[0, 2, 3])\n",
        "    channels_sq_sum += (images ** 2).sum(dim=[0, 2, 3])\n",
        "\n",
        "total_pixels = IMAGE_SIZE * IMAGE_SIZE * NUMBER_OF_CLASSES * NUMBER_OF_TRAIN_IMAGES_PER_CLASS\n",
        "normalization_mean = channels_sum / total_pixels\n",
        "normalization_var = (channels_sq_sum / total_pixels) - normalization_mean**2\n",
        "normalization_std = torch.sqrt(normalization_var)\n",
        "\n",
        "print(normalization_mean)\n",
        "print(normalization_var)\n",
        "print(normalization_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP5nn6ldgwfD"
      },
      "outputs": [],
      "source": [
        "# calculated values are assigned manually to avoid recalculation\n",
        "NORMALIZATION_MEAN = [0.4469, 0.4574, 0.4104]\n",
        "NORMALIZATION_STD = [0.2720, 0.2576, 0.2663]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WazJlDYoRO-"
      },
      "outputs": [],
      "source": [
        "# train transformations\n",
        "train_transformations = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(degrees=15),\n",
        "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    torchvision.transforms.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(NORMALIZATION_MEAN, NORMALIZATION_STD),\n",
        "])\n",
        "\n",
        "validation_transformations = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
        "  torchvision.transforms.ToTensor(),\n",
        "  torchvision.transforms.Normalize(NORMALIZATION_MEAN, NORMALIZATION_STD),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgbeJuOiXgLg"
      },
      "outputs": [],
      "source": [
        "# initialize train device\n",
        "train_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(train_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdtD7j2nHqDO"
      },
      "outputs": [],
      "source": [
        "# define datasets and dataloaders\n",
        "train_dataset = torchvision.datasets.ImageFolder(root=\"data/train/\", transform=train_transformations)\n",
        "validation_dataset = torchvision.datasets.ImageFolder(root=\"data/validation/\", transform=validation_transformations)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUMBER_OF_WORKERS, shuffle=True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=BATCH_SIZE, num_workers=NUMBER_OF_WORKERS, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for images, labels in train_dataloader:\n",
        "    print(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SjKEWh0K91Q",
        "outputId": "c824951a-2bd4-40ab-d8fa-97f2746cb4b5"
      },
      "outputs": [],
      "source": [
        "#initialize train model\n",
        "train_model = ConvolutionalNetwork(number_of_classes=NUMBER_OF_CLASSES)\n",
        "train_model.to(train_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Wb-kZx-gKfuE",
        "outputId": "c3754488-1147-4453-9a2e-28e6aa36bd44"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "train_losses = np.zeros(NUMBER_OF_EPOCHS)\n",
        "validation_losses = np.zeros(NUMBER_OF_EPOCHS)\n",
        "\n",
        "train_accuracies = np.zeros(NUMBER_OF_EPOCHS)\n",
        "validation_accuracies = np.zeros(NUMBER_OF_EPOCHS)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=train_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(NUMBER_OF_EPOCHS):\n",
        "  train_loss_acum = np.array([], dtype = np.float32)\n",
        "  validation_loss_acum = np.array([], dtype = np.float32)\n",
        "\n",
        "  correct_train = 0\n",
        "  total_train = 0\n",
        "\n",
        "  correct_validation = 0\n",
        "  total_validation = 0\n",
        "\n",
        "  #train\n",
        "  train_model.train()\n",
        "  for images, labels in train_dataloader:\n",
        "    images = images.to(train_device)\n",
        "    labels = labels.to(train_device)\n",
        "\n",
        "    predictions = train_model(images)\n",
        "    train_loss = loss_function(predictions, labels)\n",
        "    train_loss_acum = np.append(train_loss_acum, train_loss.cpu().detach().numpy())\n",
        "\n",
        "    preds = torch.argmax(predictions, dim=1)\n",
        "    correct_train += (preds == labels).sum().item()\n",
        "    total_train += labels.size(0)\n",
        "\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  # validate\n",
        "  train_model.eval()\n",
        "  with torch.no_grad():\n",
        "    for images, labels in validation_dataloader:\n",
        "      images = images.to(train_device)\n",
        "      labels = labels.to(train_device)\n",
        "\n",
        "      predictions = train_model(images)\n",
        "      validation_loss = loss_function(predictions, labels)\n",
        "      validation_loss_acum = np.append(validation_loss_acum, validation_loss.cpu().detach().numpy())\n",
        "\n",
        "      preds = torch.argmax(predictions, dim=1)\n",
        "      correct_validation += (preds == labels).sum().item()\n",
        "      total_validation += labels.size(0)\n",
        "\n",
        "  train_loss_mean = np.mean(train_loss_acum)\n",
        "  validation_loss_mean = np.mean(validation_loss_acum)\n",
        "\n",
        "  train_accuracy = correct_train / total_train\n",
        "  validation_accuracy = correct_validation / total_validation\n",
        "\n",
        "  train_losses[epoch] = train_loss_mean\n",
        "  validation_losses[epoch] = validation_loss_mean\n",
        "\n",
        "  train_accuracies[epoch] = train_accuracy\n",
        "  validation_accuracies[epoch] = validation_accuracy\n",
        "\n",
        "  print(f'Epoch: {epoch}, Train loss: {train_loss_mean} Validation loss: {validation_loss_mean}')\n",
        "  print(f'Epoch: {epoch}, Train accuracy: {train_accuracy} Validation accuracy: {validation_accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = np.arange(1, NUMBER_OF_EPOCHS + 1)\n",
        "\n",
        "# loss graph\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(epochs, train_losses, label='Train Loss')\n",
        "plt.plot(epochs, validation_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# accuracy graph\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
        "plt.plot(epochs, validation_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save train model\n",
        "torch.save(train_model.state_dict(), \"final_CN.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test imports\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize test device\n",
        "test_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(test_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize test model\n",
        "test_model = ConvolutionalNetwork(number_of_classes=NUMBER_OF_CLASSES)\n",
        "test_model.load_state_dict(torch.load(\"final_CN.pth\", weights_only=True))\n",
        "test_model.to(test_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test transformations\n",
        "test_transformations = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
        "  torchvision.transforms.ToTensor(),\n",
        "  torchvision.transforms.Normalize(NORMALIZATION_MEAN, NORMALIZATION_STD),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize test dataset and dataloader\n",
        "test_dataset = torchvision.datasets.ImageFolder(root=\"data/test/\", transform=test_transformations)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=NUMBER_OF_WORKERS, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "Ea4pWl3LuHFF",
        "outputId": "743d774e-272c-47d1-ad95-c8fd69bb78e4"
      },
      "outputs": [],
      "source": [
        "test_model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_dataloader:\n",
        "        images = images.to(test_device)\n",
        "        labels = labels.to(test_device)\n",
        "\n",
        "        predictions = test_model(images)\n",
        "        preds = torch.argmax(predictions, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test statistics\n",
        "cm = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_preds)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "print(sklearn.metrics.classification_report(all_labels, all_preds, target_names=CLASSES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UDf5NFuYEvD",
        "outputId": "c08c1494-44cb-4212-feb4-f21f225898ef"
      },
      "outputs": [],
      "source": [
        "# server application\n",
        "import fastapi\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import uvicorn\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "\n",
        "class ModelController(object):\n",
        "    def __init__(self) -> None:\n",
        "        self.prod_model = ConvolutionalNetwork(\n",
        "            number_of_classes=NUMBER_OF_CLASSES\n",
        "        )\n",
        "        self.prod_model.load_state_dict(\n",
        "            torch.load(\"final_CN.pth\", weights_only=True)\n",
        "        )\n",
        "        self.prod_model.eval()\n",
        "\n",
        "        self.prod_device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        self.prod_model.to(self.prod_device)\n",
        "\n",
        "        self.prod_transformations = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize(\n",
        "                    NORMALIZATION_MEAN, NORMALIZATION_STD\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def model_endpoint(\n",
        "        self,\n",
        "        image_file: fastapi.UploadFile = fastapi.File(...),\n",
        "    ) -> fastapi.Response:\n",
        "        image_bytes = image_file.file.read()\n",
        "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "        image_tensor = (\n",
        "            self.prod_transformations(image).unsqueeze(0).to(self.prod_device)\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.prod_model(image_tensor)\n",
        "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
        "            predicted_class = CLASSES[predicted_idx]\n",
        "            softmax_values = F.softmax(outputs, dim=1).squeeze().cpu().numpy()\n",
        "            prediction_mapping = {\n",
        "                class_name: float(softmax_values[i])\n",
        "                for i, class_name in enumerate(CLASSES)\n",
        "            }\n",
        "            return fastapi.responses.JSONResponse(\n",
        "                {\n",
        "                    \"Predicted class\": predicted_class,\n",
        "                    \"Predictions\": prediction_mapping,\n",
        "                }\n",
        "            )\n",
        "\n",
        "\n",
        "async def main() -> None:\n",
        "    model_controller = ModelController()\n",
        "    app = fastapi.FastAPI()\n",
        "    app.add_api_route(\n",
        "        path=\"/model\",\n",
        "        endpoint=model_controller.model_endpoint,\n",
        "        methods=[\"POST\"],\n",
        "    )\n",
        "    config = uvicorn.Config(app=app)\n",
        "    server = uvicorn.Server(config)\n",
        "    await server.serve()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    await main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cuda_torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
