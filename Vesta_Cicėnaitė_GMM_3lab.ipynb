{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4904799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from segmentation_dataset import SegmentationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f8f476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model properties\n",
    "NUMBER_OF_CLASSES = 4\n",
    "CLASSES = ['Cat', 'Horse', 'Van']\n",
    "ROOT_DATA_DIRECTORY = Path(\"segmentation_data\")\n",
    "BATCH_SIZE = 16\n",
    "NUMBER_OF_WORKERS = 4\n",
    "IN_CHANNELS = 3\n",
    "OUT_CHANNELS = 4\n",
    "FEATURES = [32, 64, 128, 256]\n",
    "\n",
    "LABELS = {\n",
    "    \"Cat\": 1,\n",
    "    \"Horse\": 2,\n",
    "    \"Van\": 3,\n",
    "}\n",
    "\n",
    "COLOURS = {\n",
    "    0: (0, 0, 0),\n",
    "    1: (53, 94, 59),\n",
    "    2: (150, 75, 0),\n",
    "    3: (176,224,230),\n",
    "}\n",
    "\n",
    "REVERSE_COLOURS = {\n",
    "    (0, 0, 0,) : 0,\n",
    "    (53, 94, 59) : 1,\n",
    "    (150, 75, 0) : 2,\n",
    "    (176,224,230) : 3\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b6a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train properties\n",
    "NUMBER_OF_EPOCHS = 80\n",
    "LEARNING_RATE = 0.001\n",
    "NUMBER_OF_TRAIN_IMAGES_PER_CLASS = 1000\n",
    "NUMBER_OF_VALIDATION_IMAGES_PER_CLASS = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a1d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test properties\n",
    "NUMBER_OF_TEST_IMAGES_PER_CLASS = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffaa2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images\n",
    "def load_images(root_directory:str, class_name: str, split: str, size: int) -> None:\n",
    "    dataset = foz.load_zoo_dataset(\n",
    "        \"open-images-v6\",\n",
    "        split=split,\n",
    "        label_types=[\"segmentations\"],\n",
    "        classes=[class_name],\n",
    "        max_samples=size,\n",
    "        dataset_name=f\"open-images-{split}-{class_name.lower()}-segmentation\",\n",
    "    )\n",
    "    for sample in dataset:\n",
    "        if sample.ground_truth is None:\n",
    "            continue\n",
    "\n",
    "        cat_detections = [\n",
    "            d for d in sample.ground_truth.detections if d.label == class_name\n",
    "        ]\n",
    "\n",
    "        sample.ground_truth.detections = cat_detections\n",
    "        sample.save()\n",
    "\n",
    "    dataset.export(\n",
    "        export_dir=f\"{root_directory}/{split}/{class_name}\",\n",
    "        dataset_type=fo.types.ImageSegmentationDirectory,\n",
    "        label_field=\"ground_truth\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1389cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images by class\n",
    "for class_name in CLASSES:\n",
    "    load_images(root_directory=ROOT_DATA_DIRECTORY, class_name=class_name, split=\"train\", size=NUMBER_OF_TRAIN_IMAGES_PER_CLASS)\n",
    "    load_images(root_directory=ROOT_DATA_DIRECTORY, class_name=class_name, split=\"validation\", size=NUMBER_OF_VALIDATION_IMAGES_PER_CLASS)\n",
    "    load_images(root_directory=ROOT_DATA_DIRECTORY, class_name=class_name, split=\"test\", size=NUMBER_OF_TEST_IMAGES_PER_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5379e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'C:\\Users\\vesta\\fiftyone\\open-images-v6\\train' if necessary\n",
      "Found 1 images, downloading the remaining 999\n",
      " 100% |███████████████████| 999/999 [5.9m elapsed, 0s remaining, 7.4 files/s]      \n",
      "Dataset info written to 'C:\\Users\\vesta\\fiftyone\\open-images-v6\\info.json'\n",
      "Loading 'open-images-v6' split 'train'\n",
      " 100% |███████████████| 1000/1000 [1.2m elapsed, 0s remaining, 14.7 samples/s]      \n",
      "Dataset 'open-images-train-van-segmentation' created\n",
      " 100% |███████████████| 1000/1000 [17.6s elapsed, 0s remaining, 58.6 samples/s]      \n",
      "Downloading split 'validation' to 'C:\\Users\\vesta\\fiftyone\\open-images-v6\\validation' if necessary\n",
      "Found 18 images, downloading the remaining 107\n",
      " 100% |███████████████████| 107/107 [32.7s elapsed, 0s remaining, 3.4 files/s]      \n",
      "Dataset info written to 'C:\\Users\\vesta\\fiftyone\\open-images-v6\\info.json'\n",
      "Loading 'open-images-v6' split 'validation'\n",
      " 100% |█████████████████| 125/125 [6.5s elapsed, 0s remaining, 16.4 samples/s]      \n",
      "Dataset 'open-images-validation-van-segmentation' created\n",
      " 100% |█████████████████| 125/125 [2.5s elapsed, 0s remaining, 51.5 samples/s]      \n",
      "Downloading split 'test' to 'C:\\Users\\vesta\\fiftyone\\open-images-v6\\test' if necessary\n",
      "Downloading 125 images\n",
      " 100% |███████████████████| 125/125 [27.2s elapsed, 0s remaining, 2.8 files/s]      \n",
      "Dataset info written to 'C:\\Users\\vesta\\fiftyone\\open-images-v6\\info.json'\n",
      "Loading 'open-images-v6' split 'test'\n",
      " 100% |█████████████████| 125/125 [6.6s elapsed, 0s remaining, 17.3 samples/s]      \n",
      "Dataset 'open-images-test-van-segmentation' created\n",
      " 100% |█████████████████| 125/125 [2.5s elapsed, 0s remaining, 49.0 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "class_name = \"Van\"\n",
    "load_images(root_directory=ROOT_DATA_DIRECTORY, class_name=class_name, split=\"train\", size=NUMBER_OF_TRAIN_IMAGES_PER_CLASS)\n",
    "load_images(root_directory=ROOT_DATA_DIRECTORY, class_name=class_name, split=\"validation\", size=NUMBER_OF_VALIDATION_IMAGES_PER_CLASS)\n",
    "load_images(root_directory=ROOT_DATA_DIRECTORY, class_name=class_name, split=\"test\", size=NUMBER_OF_TEST_IMAGES_PER_CLASS)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3da2e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coloured_masks_for_each_class(\n",
    "    classes: list[str],\n",
    "    label_map: dict[str, int],\n",
    "    colour_map: dict[int, tuple[int, int, int]],\n",
    "    root_directory: Path,\n",
    "    split: str\n",
    ") -> None:\n",
    "    for class_name in classes:\n",
    "        class_mask_dir = root_directory / split / class_name / \"labels\"\n",
    "        class_coloured_dir = root_directory / split / class_name / \"coloured_labels\"\n",
    "        class_coloured_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        class_id = label_map[class_name]\n",
    "        class_color = colour_map[class_id]\n",
    "\n",
    "        for mask_path in class_mask_dir.glob(\"*.png\"):\n",
    "            binary_mask = Image.open(mask_path).convert(\"L\")\n",
    "            mask_array = np.array(binary_mask) > 127\n",
    "\n",
    "            height, width = mask_array.shape\n",
    "            coloured_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            coloured_mask[mask_array] = class_color\n",
    "\n",
    "            out_path = class_coloured_dir / mask_path.name\n",
    "            Image.fromarray(coloured_mask).save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d58239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multiclass masks\n",
    "create_coloured_masks_for_each_class(classes=CLASSES, label_map=LABELS, colour_map=COLOURS, root_directory=ROOT_DATA_DIRECTORY, split=\"train\")\n",
    "create_coloured_masks_for_each_class(classes=CLASSES, label_map=LABELS, colour_map=COLOURS, root_directory=ROOT_DATA_DIRECTORY, split=\"validation\")\n",
    "create_coloured_masks_for_each_class(classes=CLASSES, label_map=LABELS, colour_map=COLOURS, root_directory=ROOT_DATA_DIRECTORY, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548dae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_directory \u001b[38;5;129;01min\u001b[39;00m normalization_image_directories:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m image_directory\u001b[38;5;241m.\u001b[39mrglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[0;32m     10\u001b[0m             img_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(img))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     12\u001b[0m         channels_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m img_tensor\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\vesta\\anaconda3\\envs\\cuda_torch\\lib\\site-packages\\PIL\\Image.py:984\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    982\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vesta\\anaconda3\\envs\\cuda_torch\\lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calculate normalization values for train dataset\n",
    "normalization_image_directories = [ROOT_DATA_DIRECTORY / \"train\" / class_name / \"data\" for class_name in CLASSES]\n",
    "\n",
    "channels_sum = torch.zeros(3)\n",
    "channels_sq_sum = torch.zeros(3)\n",
    "total_pixels = 0\n",
    "\n",
    "for image_directory in normalization_image_directories:\n",
    "    for img_path in image_directory.rglob(\"*\"):\n",
    "        with Image.open(img_path).convert(\"RGB\") as img:\n",
    "            img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        channels_sum += img_tensor.sum(dim=[1, 2])\n",
    "        channels_sq_sum += (img_tensor ** 2).sum(dim=[1, 2])\n",
    "\n",
    "        height, width = img_tensor.shape[1], img_tensor.shape[2]\n",
    "        total_pixels += height * width\n",
    "\n",
    "mean = channels_sum / total_pixels\n",
    "var = (channels_sq_sum / total_pixels) - mean**2\n",
    "std = torch.sqrt(var)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"STD:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e79317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated values are assigned manually to avoid recalculation\n",
    "NORMALIZATION_MEAN = [0.4650, 0.4405, 0.4009]\n",
    "NORMALIZATION_STD = [0.2812, 0.2759, 0.2809]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7cad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationConvolutionalNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, features: list[int]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool_layer = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.down_layers = torch.nn.ModuleList()\n",
    "        current_in_channels = in_channels\n",
    "        for feature in features:\n",
    "            self.down_layers.append(\n",
    "                SegmentationConvolutionalNetwork.double_convolution(\n",
    "                    in_channels=current_in_channels,\n",
    "                    out_channels=feature\n",
    "                )\n",
    "            )\n",
    "            current_in_channels = feature\n",
    "        \n",
    "        self.bottleneck_layer = SegmentationConvolutionalNetwork.double_convolution(in_channels=features[-1], out_channels=features[-1]*2)\n",
    "\n",
    "        self.up_layers = torch.nn.ModuleList()\n",
    "        current_in_channels = features[-1] * 2\n",
    "        for feature in reversed(features):\n",
    "            self.up_layers.append(\n",
    "                torch.nn.ConvTranspose2d(\n",
    "                    in_channels=current_in_channels,\n",
    "                    out_channels=feature,\n",
    "                    kernel_size=2,\n",
    "                    stride=2\n",
    "                )\n",
    "            )\n",
    "            self.up_layers.append(\n",
    "                SegmentationConvolutionalNetwork.double_convolution(\n",
    "                    in_channels=feature * 2,\n",
    "                    out_channels=feature\n",
    "                )\n",
    "            )\n",
    "            current_in_channels = feature\n",
    "\n",
    "        self.final_convolution = torch.nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def double_convolution(cls, in_channels: int, out_channels: int):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(num_features=out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(num_features=out_channels),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        skip_connections = []\n",
    "\n",
    "        out = x\n",
    "\n",
    "        for down_layer in self.down_layers:\n",
    "            out = down_layer(out)\n",
    "            skip_connections.append(out)\n",
    "            out = self.pool_layer(out)\n",
    "        \n",
    "        out = self.bottleneck_layer(out)\n",
    "\n",
    "        reversed_skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for layer_id in range(0, len(self.up_layers), 2):\n",
    "            transposed_convolution = self.up_layers[layer_id]\n",
    "            out = transposed_convolution(out)\n",
    "\n",
    "            skip_out = reversed_skip_connections[layer_id // 2]\n",
    "\n",
    "            if out.shape != skip_out.shape:\n",
    "                out = torch.nn.functional.interpolate(out, size=skip_out.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            out = torch.cat([skip_out, out], dim=1)\n",
    "\n",
    "            out = self.up_layers[layer_id + 1](out)\n",
    "        \n",
    "        out = self.final_convolution(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f749d542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# initialize train device\n",
    "train_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(train_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4256279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegmentationConvolutionalNetwork(\n",
       "  (pool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (down_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (bottleneck_layer): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (up_layers): ModuleList(\n",
       "    (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (6): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (final_convolution): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize train model\n",
    "train_model = SegmentationConvolutionalNetwork(in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=FEATURES)\n",
    "train_model.to(train_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9c472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation_data\n",
      "segmentation_data\n"
     ]
    }
   ],
   "source": [
    "# train and validation datasets\n",
    "train_dataset = SegmentationDataset(classes=CLASSES, root_directory=ROOT_DATA_DIRECTORY, mode=\"train\", reverse_colours=REVERSE_COLOURS, normalization_mean=NORMALIZATION_MEAN, normalization_std=NORMALIZATION_STD)\n",
    "validation_dataset = SegmentationDataset(classes=CLASSES, root_directory=ROOT_DATA_DIRECTORY, mode=\"validation\", reverse_colours=REVERSE_COLOURS, normalization_mean=NORMALIZATION_MEAN, normalization_std=NORMALIZATION_STD)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUMBER_OF_WORKERS, shuffle=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1, num_workers=NUMBER_OF_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d1176cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_iou(preds:torch.Tensor, labels: torch.Tensor, number_of_classes:int) -> float:\n",
    "    if preds.dim() == 4 and preds.size(1) > 1:\n",
    "        preds = preds.argmax(dim=1) \n",
    "    \n",
    "    preds_flat = preds.view(-1)\n",
    "    labels_flat = labels.view(-1)\n",
    "\n",
    "    ious = []\n",
    "    for c in range(number_of_classes):\n",
    "        pred_c = (preds_flat == c)\n",
    "        label_c = (labels_flat == c)\n",
    "\n",
    "        intersection = (pred_c & label_c).sum().item()\n",
    "        union = (pred_c | label_c).sum().item()\n",
    "\n",
    "        if union == 0:\n",
    "            continue\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "\n",
    "    if len(ious) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return np.mean(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243623a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int, softmax_dim: int = 1, smooth: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.softmax_dim = softmax_dim\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        probs = torch.nn.functional.softmax(inputs, dim=self.softmax_dim)\n",
    "        targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=self.num_classes)\n",
    "        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()\n",
    "\n",
    "        assert probs.shape == targets_one_hot.shape, \\\n",
    "            f\"Input ({probs.shape}) and target ({targets_one_hot.shape}) shapes do not match.\"\n",
    "\n",
    "        probs_flat = probs.view(probs.size(0), probs.size(1), -1)\n",
    "        targets_flat = targets_one_hot.view(targets_one_hot.size(0), targets_one_hot.size(1), -1)\n",
    "\n",
    "        intersection = (probs_flat * targets_flat).sum(dim=2)\n",
    "        cardinality = (probs_flat + targets_flat).sum(dim=2)\n",
    "\n",
    "        dice_score = ((2. * intersection + self.smooth) / (cardinality + self.smooth)).mean(dim=0)\n",
    "\n",
    "        dice_score_mean = dice_score.mean()\n",
    "\n",
    "        dice_loss = 1.0 - dice_score_mean\n",
    "\n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54e3f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vesta\\anaconda3\\envs\\cuda_torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 0, Train loss: 0.7244798541069031 Validation loss: 0.7187759876251221\n",
      "Epoch: 0, Train IoU: 0.19725064004431567 Validation IoU: 0.33556737738343073\n",
      "1\n",
      "Epoch: 1, Train loss: 0.7180646657943726 Validation loss: 0.7217940092086792\n",
      "Epoch: 1, Train IoU: 0.20053024187998292 Validation IoU: 0.3793187879283872\n",
      "2\n",
      "Epoch: 2, Train loss: 0.710625171661377 Validation loss: 0.7191142439842224\n",
      "Epoch: 2, Train IoU: 0.2022814317197331 Validation IoU: 0.3488809162275592\n",
      "3\n",
      "Epoch: 3, Train loss: 0.719700276851654 Validation loss: 0.7092809081077576\n",
      "Epoch: 3, Train IoU: 0.20343012119787204 Validation IoU: 0.37498430945463496\n",
      "4\n",
      "Epoch: 4, Train loss: 0.7151015996932983 Validation loss: 0.7053115367889404\n",
      "Epoch: 4, Train IoU: 0.20144179896210349 Validation IoU: 0.3745417185974053\n",
      "5\n",
      "Epoch: 5, Train loss: 0.7107389569282532 Validation loss: 0.7061546444892883\n",
      "Epoch: 5, Train IoU: 0.20238922631178943 Validation IoU: 0.32562885560656074\n",
      "6\n",
      "Epoch: 6, Train loss: 0.7112095355987549 Validation loss: 0.7048212885856628\n",
      "Epoch: 6, Train IoU: 0.20492649881597566 Validation IoU: 0.3085395348341769\n",
      "7\n",
      "Epoch: 7, Train loss: 0.7104609608650208 Validation loss: 0.7109931707382202\n",
      "Epoch: 7, Train IoU: 0.21225486135409155 Validation IoU: 0.3735418917243997\n",
      "8\n",
      "Epoch: 8, Train loss: 0.7112162113189697 Validation loss: 0.7045103311538696\n",
      "Epoch: 8, Train IoU: 0.21397628850118788 Validation IoU: 0.3227446140372292\n",
      "9\n",
      "Epoch: 9, Train loss: 0.7118144035339355 Validation loss: 0.6993994116783142\n",
      "Epoch: 9, Train IoU: 0.2210595234908864 Validation IoU: 0.3361243964827114\n",
      "10\n",
      "Epoch: 10, Train loss: 0.7059104442596436 Validation loss: 0.7110553979873657\n",
      "Epoch: 10, Train IoU: 0.2227563443919773 Validation IoU: 0.32473824327190376\n",
      "11\n",
      "Epoch: 11, Train loss: 0.6980712413787842 Validation loss: 0.7007143497467041\n",
      "Epoch: 11, Train IoU: 0.22672486935641933 Validation IoU: 0.3279260032337414\n",
      "12\n",
      "Epoch: 12, Train loss: 0.6947862505912781 Validation loss: 0.708332359790802\n",
      "Epoch: 12, Train IoU: 0.23789852655567145 Validation IoU: 0.3312898669829559\n",
      "13\n",
      "Epoch: 13, Train loss: 0.6996434926986694 Validation loss: 0.7101054787635803\n",
      "Epoch: 13, Train IoU: 0.23684874810976111 Validation IoU: 0.3418400445885397\n",
      "14\n",
      "Epoch: 14, Train loss: 0.694308340549469 Validation loss: 0.6933119893074036\n",
      "Epoch: 14, Train IoU: 0.2521284851623771 Validation IoU: 0.35495416144821373\n",
      "15\n",
      "Epoch: 15, Train loss: 0.6820341348648071 Validation loss: 0.688479483127594\n",
      "Epoch: 15, Train IoU: 0.25686742996954715 Validation IoU: 0.35970453675680764\n",
      "16\n",
      "Epoch: 16, Train loss: 0.683677613735199 Validation loss: 0.6846878528594971\n",
      "Epoch: 16, Train IoU: 0.25653414132951396 Validation IoU: 0.36015832526773056\n",
      "17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 49\u001b[0m     iou_batch \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mean_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUMBER_OF_CLASSES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     train_iou_acum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(train_iou_acum, iou_batch)\n\u001b[0;32m     52\u001b[0m train_model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m, in \u001b[0;36mcompute_mean_iou\u001b[1;34m(preds, labels, number_of_classes)\u001b[0m\n\u001b[0;32m     10\u001b[0m pred_c \u001b[38;5;241m=\u001b[39m (preds_flat \u001b[38;5;241m==\u001b[39m c)\n\u001b[0;32m     11\u001b[0m label_c \u001b[38;5;241m=\u001b[39m (labels_flat \u001b[38;5;241m==\u001b[39m c)\n\u001b[1;32m---> 13\u001b[0m intersection \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpred_c\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel_c\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m union \u001b[38;5;241m=\u001b[39m (pred_c \u001b[38;5;241m|\u001b[39m label_c)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m union \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_losses = np.zeros(NUMBER_OF_EPOCHS)\n",
    "validation_losses = np.zeros(NUMBER_OF_EPOCHS)\n",
    "\n",
    "train_ious = np.zeros(NUMBER_OF_EPOCHS)\n",
    "validation_ious = np.zeros(NUMBER_OF_EPOCHS)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=train_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loss_function_ce = torch.nn.CrossEntropyLoss()\n",
    "loss_function_dice = DiceLoss(num_classes=NUMBER_OF_CLASSES)\n",
    "\n",
    "ce_weight = 0.5\n",
    "dice_weight = 0.5\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',    \n",
    "    factor=0.1,    \n",
    "    patience=3,     \n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    train_loss_acum = np.array([], dtype = np.float32)\n",
    "    validation_loss_acum = np.array([], dtype = np.float32)\n",
    "\n",
    "    train_iou_acum = np.array([], dtype = np.float32)\n",
    "    validation_iou_acum = np.array([], dtype = np.float32)\n",
    "\n",
    "    train_model.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to(train_device)\n",
    "        labels = labels.to(train_device)\n",
    "\n",
    "        predictions = train_model(images)\n",
    "        ce_loss_train = loss_function_ce(predictions, labels)\n",
    "        dice_loss_train = loss_function_dice(predictions, labels)\n",
    "        total_loss_train = ce_weight * ce_loss_train + dice_weight * dice_loss_train\n",
    "        train_loss_acum = np.append(train_loss_acum, total_loss_train.cpu().detach().numpy())\n",
    "\n",
    "        total_loss_train.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        iou_batch = compute_mean_iou(predictions.detach(), labels, number_of_classes=NUMBER_OF_CLASSES)\n",
    "        train_iou_acum = np.append(train_iou_acum, iou_batch)\n",
    "\n",
    "    train_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_dataloader:\n",
    "            images = images.to(train_device)\n",
    "            labels = labels.to(train_device)\n",
    "\n",
    "            predictions = train_model(images)\n",
    "\n",
    "            ce_loss_validation = loss_function_ce(predictions, labels)\n",
    "            dice_loss_validation = loss_function_dice(predictions, labels)\n",
    "            total_loss_validation = ce_weight * ce_loss_validation + dice_weight * dice_loss_validation\n",
    "            validation_loss_acum = np.append(validation_loss_acum, total_loss_validation.cpu().detach().numpy())\n",
    "\n",
    "            iou_batch = compute_mean_iou(preds=predictions.detach(), labels=labels, number_of_classes=NUMBER_OF_CLASSES)\n",
    "            validation_iou_acum = np.append(validation_iou_acum, iou_batch)\n",
    "    \n",
    "    train_losses[epoch] = np.mean(train_loss_acum)\n",
    "    train_ious[epoch]   = np.mean(train_iou_acum)\n",
    "\n",
    "    validation_losses[epoch] = np.mean(validation_loss_acum)\n",
    "    validation_ious[epoch] = np.mean(validation_iou_acum)\n",
    "\n",
    "    scheduler.step(validation_losses[epoch])\n",
    "\n",
    "    print(f'Epoch: {epoch}, Train loss: {train_losses[epoch]} Validation loss: {validation_losses[epoch]}')\n",
    "    print(f'Epoch: {epoch}, Train IoU: {train_ious[epoch]} Validation IoU: {validation_ious[epoch]}')\n",
    "\n",
    "    torch.save(train_model.state_dict(), f\"segmentation_models/{epoch}_segmentation_CN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation graphs\n",
    "epochs = np.arange(1, NUMBER_OF_EPOCHS + 1)\n",
    "\n",
    "# loss graph\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# iou graph\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epochs, train_ious, label='Train Intersection Over Union')\n",
    "plt.plot(epochs, validation_ious, label='Validation Intersection Over Union')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Intersection Over Union')\n",
    "plt.title('Intersection Over Union over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2763985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [16156]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62621 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62630 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62634 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62636 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62638 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62643 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62645 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62646 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62648 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62651 - \"POST /model HTTP/1.1\" 200 OK\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:     127.0.0.1:62653 - \"POST /model HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [16156]\n"
     ]
    }
   ],
   "source": [
    "# server application\n",
    "import io\n",
    "\n",
    "import fastapi\n",
    "import torch\n",
    "import torchvision\n",
    "import uvicorn\n",
    "from PIL import Image\n",
    "\n",
    "class ModelController(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.prod_device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.prod_device)\n",
    "\n",
    "        self.prod_model = SegmentationConvolutionalNetwork(in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS, features=FEATURES)\n",
    "        self.prod_model.load_state_dict(\n",
    "            torch.load(\"segmentation_models/first_segmentation_CN.pth\", weights_only=True)\n",
    "        )\n",
    "        self.prod_model.to(device=self.prod_device)\n",
    "        self.prod_model.eval()\n",
    "\n",
    "        self.prod_transformations = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    NORMALIZATION_MEAN, NORMALIZATION_STD\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def model_endpoint(self, image_file: fastapi.UploadFile = fastapi.File(...), ) -> fastapi.Response:\n",
    "        image_bytes = image_file.file.read()\n",
    "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        image_tensor = (\n",
    "            self.prod_transformations(image).unsqueeze(0).to(self.prod_device)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            predictions = self.prod_model(image_tensor)\n",
    "\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "        print(predicted_classes)\n",
    "        prediction_map = predicted_classes.squeeze(0).cpu().numpy()\n",
    "        height, width = prediction_map.shape\n",
    "        mask_image_np = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "        for class_index, color in COLOURS.items():\n",
    "            mask_image_np[prediction_map == class_index] = color\n",
    "\n",
    "        mask_image_pil = Image.fromarray(mask_image_np)\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        mask_image_pil.save(buffer, format=\"PNG\")\n",
    "        buffer.seek(0)\n",
    "\n",
    "        return fastapi.Response(content=buffer.getvalue(), media_type=\"image/png\")\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    model_controller = ModelController()\n",
    "    app = fastapi.FastAPI()\n",
    "    app.add_api_route(\n",
    "        path=\"/model\",\n",
    "        endpoint=model_controller.model_endpoint,\n",
    "        methods=[\"POST\"],\n",
    "    )\n",
    "    config = uvicorn.Config(app=app)\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
